{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import warnings\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import pickle as pkl\n",
    "from importlib import reload\n",
    "import cv2\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "# dimensionality reduction\n",
    "from sklearn.manifold import trustworthiness\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6jS_AcXrKt-"
   },
   "source": [
    "# Loading Image Paths and Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQNqVpfQrViW"
   },
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open(\"config.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "# Set base directory path\n",
    "base_dir = config[\"project_paths\"][\"base_dir\"]\n",
    "\n",
    "# Images\n",
    "image_dir = config[\"project_paths\"]['images'] # path to the images\n",
    "filenames = os.listdir(image_dir) # list of all images in the directory\n",
    "\n",
    "# Sprite Images\n",
    "sprite = os.path.join(base_dir, 'outputs/sprite_image.png')\n",
    "\n",
    "# Feature Vector Dictionaries (img_filename:feature_vector)\n",
    "feature_vector_path = os.path.join(base_dir, 'data/feature_vectors_dict_DM_full_dino_48_augmentations_05092024.pt')\n",
    "dino_dict = torch.load(feature_vector_path)\n",
    "\n",
    "# model name\n",
    "model_name = 'dino'\n",
    "\n",
    "# dictionary of feature_vectors dictionaries - {img_filename:feature_vector}\n",
    "model_dict = {'dict': dino_dict,\n",
    "                   'filenames': filenames,\n",
    "                   'image_dir': image_dir,\n",
    "                   'sprite': sprite\n",
    "                  }\n",
    "\n",
    "# getting dictionary of feature vectors in order of filenames\n",
    "fv = {} # feature vectors\n",
    "\n",
    "feat_dict = model_dict['dict']\n",
    "vectors = [feat_dict[filename] for filename in model_dict['filenames'] if filename.endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "fv[model_name] = vectors\n",
    "\n",
    "# L2 normalization\n",
    "fv[\"l2\"] = preprocessing.normalize(fv[\"dino\"], norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Sprite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (50, 50)  # Size to which each image will be resized\n",
    "\n",
    "#sprite = os.path.join(base_dir, 'outputs/sprite_image_150.png')\n",
    "#utils.create_sprite(filenames, image_dir, sprite, image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv_reduced = {}\n",
    "fv_whitened = {}\n",
    "fv_explained_variance = {}\n",
    "\n",
    "variance_threshold = [0.9]\n",
    "\n",
    "# Print Individual Explained Variance Graphs\n",
    "for name, vectors in fv.items():\n",
    "        print(name)\n",
    "        print('Number of feature vectors:', len(vectors))\n",
    "        print(np.array(vectors).shape)\n",
    "\n",
    "        for variance in variance_threshold:\n",
    "\n",
    "                (vectors_reduced,\n",
    "                vectors_whitened,\n",
    "                explained_variance,\n",
    "                num_components) = utils.print_pca_variations(vectors,\n",
    "                                                        variance_threshold=variance,\n",
    "                                                        show=True,\n",
    "                                                        ratio=False)\n",
    "\n",
    "                variance_name = '0_' + str(np.round(variance, 2)).split('.')[-1]\n",
    "\n",
    "                fv_reduced[name] = vectors_reduced\n",
    "                fv_whitened[name] = vectors_whitened\n",
    "                fv_explained_variance[name] = explained_variance\n",
    "\n",
    "print(num_components)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for idx, (name, explained_variance) in enumerate(fv_explained_variance.items()):\n",
    "        cumulative_variance = np.cumsum(explained_variance)  # Compute the cumulative sum of explained variance ratios\n",
    "\n",
    "        plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance * 100, where='mid', label=f'DINOv2 Dreamachine Features')\n",
    "        plt.axhline(y=variance_threshold[0]*100, color='green', label=f\"Variance Threshold: {variance_threshold[0] * 100:.1f}%\")\n",
    "        plt.axvline(x=num_components, ymax=variance_threshold[0], color='purple', ls='--', label=f'Components Retained: {num_components}')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.xlim(0, 1024)\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.xlabel('principal components')\n",
    "        plt.ylabel('percent variance explained')\n",
    "\n",
    "print(fv_reduced.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Trustworthiness looking at Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_umap_embeddings(\n",
    "        vectors,\n",
    "        components,\n",
    "        n_neighbors_range,\n",
    "        trustworthiness_nn,\n",
    "        metric=\"cosine\",\n",
    "        random_state=None):\n",
    "\n",
    "    for n_neighbors in n_neighbors_range:\n",
    "\n",
    "        print(\"NN:\", n_neighbors)\n",
    "        print(50*\"-\")\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.ylabel(f\"Trustworthiness\")\n",
    "        plt.xlabel(\"UMAP Components\")\n",
    "        plt.title(\"UMAP embeddings: n_neighbors=\" + str(n_neighbors))\n",
    "        plt.grid(True)\n",
    "\n",
    "        for trust_nn in trustworthiness_nn:\n",
    "\n",
    "            print(\"trust n_neighbors:\", trust_nn)\n",
    "\n",
    "            embeddings, trust_scores = {}, {}\n",
    "\n",
    "            for c in components:\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "                    umap_model = umap.UMAP(\n",
    "                        n_components=c,\n",
    "                        n_neighbors=n_neighbors,\n",
    "                        min_dist=0.0,\n",
    "                        metric=metric,\n",
    "                        random_state=random_state,\n",
    "                    ).fit_transform(vectors)\n",
    "\n",
    "                embeddings[c] = umap_model\n",
    "                trust_scores[c] = trustworthiness(np.asarray(vectors), umap_model, n_neighbors=trust_nn)\n",
    "\n",
    "            plt.plot(list(trust_scores.keys()),\n",
    "                    list(trust_scores.values()),\n",
    "                    marker=\"o\", label=trust_nn)\n",
    "\n",
    "        plt.legend(title=\"Trustworthiness Parameter: NN\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "vectors = fv[\"l2\"]\n",
    "\n",
    "print(vectors.shape)\n",
    "\n",
    "evaluate_umap_embeddings(vectors,\n",
    "                        components=range(2, 10),\n",
    "                        trustworthiness_nn=[25, 50, 75, 100, 125, 150],\n",
    "                        n_neighbors_range=[25, 50, 75, 100, 125, 150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Trustworthiness looking at NNeighbors (UMAP Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_umap_embeddings(\n",
    "        vectors,\n",
    "        components,\n",
    "        n_neighbors_range,\n",
    "        trustworthiness_nn,\n",
    "        metric=\"euclidean\",\n",
    "        random_state=None):\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.ylabel(f\"Trustworthiness\")\n",
    "    plt.xlabel(\"n_neighbors\")\n",
    "    plt.title(\"UMAP embeddings\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    for trust_nn in trustworthiness_nn:\n",
    "\n",
    "        embeddings, trust_scores = {}, {}\n",
    "\n",
    "        for n_neighbors in n_neighbors_range:\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "\n",
    "                umap_model = umap.UMAP(\n",
    "                    n_components=components,\n",
    "                    n_neighbors=n_neighbors,\n",
    "                    min_dist=0.0,\n",
    "                    metric=metric,\n",
    "                    random_state=random_state,\n",
    "                ).fit_transform(vectors)\n",
    "\n",
    "            embeddings[n_neighbors] = umap_model\n",
    "            trust_scores[n_neighbors] = trustworthiness(np.asarray(vectors), umap_model, n_neighbors=trust_nn)\n",
    "\n",
    "        plt.plot(list(trust_scores.keys()),\n",
    "                list(trust_scores.values()),\n",
    "                marker=\"o\", label=trust_nn)\n",
    "\n",
    "    plt.legend(title=\"Trustworthiness Parameter: NN\")\n",
    "    plt.show()\n",
    "\n",
    "evaluate_umap_embeddings(fv[\"l2\"],\n",
    "                        components=6,\n",
    "                        trustworthiness_nn=[25, 50, 75, 100, 125, 150],\n",
    "                        n_neighbors_range=[25, 50, 75, 100, 125, 150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = fv[\"l2\"]\n",
    "\n",
    "\"\"\"# Save UMAP embeddings (6D, 100NN)\n",
    "umap_model = umap.UMAP(\n",
    "    n_components=6,\n",
    "    n_neighbors=100,\n",
    "    min_dist=0.0,\n",
    "    random_state=42,\n",
    "    metric=\"cosine\",\n",
    "    ).fit_transform(vectors)\n",
    "\n",
    "umap_2d = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=100,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    ).fit_transform(vectors)\"\"\"\n",
    "\n",
    "umap_2d_150 = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=150,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    ).fit_transform(vectors)\n",
    "\n",
    "path_to_save = base_dir + \"/outputs\"\n",
    "\"\"\"\n",
    "filename = \"umap_embeddings_6d_100nn.pkl\"\n",
    "with open(os.path.join(path_to_save, filename), \"wb\") as file:\n",
    "    pkl.dump(umap_model, file)\n",
    "\n",
    "filename = \"umap_embeddings_2d_100nn.pkl\"\n",
    "with open(os.path.join(path_to_save, filename), \"wb\") as file:\n",
    "    pkl.dump(umap_2d, file)\"\"\"\n",
    "\n",
    "filename = \"umap_embeddings_2d_150nn.pkl\"\n",
    "with open(os.path.join(path_to_save, filename), \"wb\") as file:\n",
    "    pkl.dump(umap_2d_150, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pca_projection(vectors, model_dict, show_images):\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        vectors_pca = pca.fit(np.array(vectors)).transform(np.array(vectors))\n",
    "\n",
    "        if show_images:\n",
    "                utils.plot_with_images(vectors_pca,\n",
    "                                       model_dict)\n",
    "        else:\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.scatter(vectors_pca[:, 0], vectors_pca[:, 1])\n",
    "                plt.xlabel('PCA Dimension 1')\n",
    "                plt.ylabel('PCA Dimension 2')\n",
    "                plt.title('PCA Visualization')\n",
    "                plt.show()\n",
    "\n",
    "        return vectors_pca\n",
    "\n",
    "pca_projection = {}\n",
    "\n",
    "for name, vectors in fv.items():\n",
    "    print(name)\n",
    "\n",
    "    pca_projection[name] = print_pca_projection(vectors, model_dict, show_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdLNL-wFGG3y"
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap(vectors,\n",
    "              model_dict,\n",
    "              plot2d = True,\n",
    "              show_images=False,\n",
    "              zoom=0.3,\n",
    "              n_neighbors=50,\n",
    "              min_dist=0.0,\n",
    "              metric=\"cosine\",\n",
    "              clusters=None,\n",
    "              figsize=(30,30),\n",
    "              subsample_size=5000\n",
    "              ):\n",
    "\n",
    "    umap_2d = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    X_umap_2d = umap_2d.fit_transform(np.array(vectors))\n",
    "\n",
    "    if plot2d:\n",
    "        print(f\"Trustworthiness: {trustworthiness(vectors, X_umap_2d, n_neighbors=30):.2f}\")\n",
    "\n",
    "        if show_images:\n",
    "            utils.plot_with_images(X_umap_2d, model_dict, figsize=figsize, zoom=zoom, cleaned=False, subsample_size=subsample_size)\n",
    "\n",
    "        else:\n",
    "            plt.figure(figsize=(20, 20))\n",
    "            plt.scatter(X_umap_2d[:, 0], X_umap_2d[:, 1])\n",
    "            plt.xlabel('UMAP Dimension 1')\n",
    "            plt.ylabel('UMAP Dimension 2')\n",
    "            plt.title('UMAP Visualization')\n",
    "            plt.show()\n",
    "\n",
    "    return X_umap_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.cluster import HDBSCAN\n",
    "import hdbscan\n",
    "from hdbscan.validity import validity_index\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "vectors = fv_reduced[\"l2\"]\n",
    "\n",
    "clusters_dict = defaultdict(lambda: defaultdict(dict))  # {Cluster count: {n_neighbors: {min_samples: (cluster_labels)}}}\n",
    "\n",
    "umap2d = plot_umap(vectors,\n",
    "                model_dict,\n",
    "                show_images=True,\n",
    "                zoom=0.3,\n",
    "                n_neighbors=100,\n",
    "                min_dist=0,\n",
    "                clusters=None,\n",
    "                figsize=(30,30),\n",
    "                subsample_size=5000\n",
    "    )\n",
    "\n",
    "for n_neighbours in [100]:\n",
    "\n",
    "    print('n_neighbours: ', n_neighbours)\n",
    "\n",
    "    umap_model = umap.UMAP(\n",
    "    n_components=6,\n",
    "    n_neighbors=n_neighbours,\n",
    "    min_dist=0,\n",
    "    random_state=42,\n",
    "    metric=\"cosine\"\n",
    "    ).fit_transform(vectors)\n",
    "\n",
    "    # umap_embedding is the 2‑D array returned by UMAP\n",
    "    cache_dir = 'data/hdbscan_cache'\n",
    "    mem = joblib.Memory(location=cache_dir, verbose=0)\n",
    "\n",
    "    umap_model = np.asarray(umap_model, dtype=np.float64)\n",
    "\n",
    "    outlier_size, num_clusters, dbcv = {}, {}, {}\n",
    "    max_clusters = 0\n",
    "    min_outliers = len(vectors)\n",
    "\n",
    "    min_cluster_size = 99\n",
    "\n",
    "    for min_samples in tqdm(range(1, 100, 1)):\n",
    "        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples,\n",
    "                                        metric=\"cosine\", algorithm=\"generic\", memory=mem).fit(umap_model)\n",
    "        outlier_size[min_samples+1] = np.sum(hdbscan_model.labels_ == -1)\n",
    "        num_clusters[min_samples+1] = len(set(hdbscan_model.labels_)) - 1\n",
    "\n",
    "        # Compute DBCV\n",
    "        dbcv[min_samples+1] = validity_index(umap_model, hdbscan_model.labels_, metric=\"cosine\")\n",
    "\n",
    "        # Adding to parameter dictionary for stability comparision\n",
    "        clusters_dict[num_clusters[min_samples+1]][n_neighbours][min_samples+1] = hdbscan_model.labels_\n",
    "\n",
    "        # Finding Optimal Solution\n",
    "        if num_clusters[min_samples+1] > max_clusters:\n",
    "            max_clusters = num_clusters[min_samples+1]\n",
    "            min_outliers = outlier_size[min_samples+1]\n",
    "            best_min_samples = min_samples+1\n",
    "        elif num_clusters[min_samples+1] == max_clusters:\n",
    "            if outlier_size[min_samples+1] < min_outliers:\n",
    "                min_outliers = outlier_size[min_samples+1]\n",
    "                best_min_samples = min_samples+1\n",
    "\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=best_min_samples, metric=\"cosine\").fit(umap_model)\n",
    "\n",
    "    print('Max clusters: ', max_clusters)\n",
    "    print('Min outliers: ', min_outliers)\n",
    "\n",
    "    print('Max/min # cluster: ', np.max(list(num_clusters.values())), np.min(list(num_clusters.values())))\n",
    "    print('Max/min outlier size: ', np.max(list(outlier_size.values())), np.min(list(outlier_size.values())))\n",
    "\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(outlier_size.keys(), outlier_size.values(), marker='o')\n",
    "    plt.axvline(x=22, color='red', linestyle='--', label=\"Selected Solution: \\n min_sample=22\")\n",
    "    plt.ylabel('Unclustered (Noise) Data Points')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(num_clusters.keys(), num_clusters.values(), marker='o')\n",
    "    plt.axvline(x=22, color='red', linestyle='--')\n",
    "    plt.ylabel('Number of Clusters')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(dbcv.keys(), dbcv.values(), marker='o')\n",
    "    plt.axvline(x=22, color='red', linestyle='--')\n",
    "    plt.xlabel('Min Samples')\n",
    "    plt.ylabel('DBCV Score')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    utils.print_projection(hdbscan_model.labels_, model_dict, umap2d, show_images=False, clusters_per_marker=None, zoom=0.4)\n",
    "    utils.print_projection(hdbscan_model.labels_, model_dict, umap2d, show_images=True, clusters_per_marker=None, zoom=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "def _jaccard_coassignment(labels_a, labels_b, ignore_noise=True, noise_label=-1):\n",
    "    \"\"\"\n",
    "    Jaccard index over *pairwise co-assignment*:\n",
    "    J = |pairs clustered together in both| / |pairs clustered together in either|\n",
    "    Optionally ignores any pair where at least one element is labeled as noise.\n",
    "    \"\"\"\n",
    "    labels_a = np.asarray(labels_a)\n",
    "    labels_b = np.asarray(labels_b)\n",
    "    n = labels_a.shape[0]\n",
    "    # mask for non-noise items if requested\n",
    "    if ignore_noise:\n",
    "        ok = (labels_a != noise_label) & (labels_b != noise_label)\n",
    "    else:\n",
    "        ok = np.ones(n, dtype=bool)\n",
    "    idx = np.where(ok)[0]\n",
    "    if len(idx) < 2:\n",
    "        return np.nan  # not enough points to form pairs\n",
    "\n",
    "    # For efficiency, compute co-assignment via label equality on an index subset\n",
    "    la = labels_a[idx]\n",
    "    lb = labels_b[idx]\n",
    "    # co-assignment boolean matrices (upper triangle implied)\n",
    "    same_a = la[:, None] == la[None, :]\n",
    "    same_b = lb[:, None] == lb[None, :]\n",
    "    # use upper triangle without diagonal\n",
    "    iu = np.triu_indices(len(idx), k=1)\n",
    "    A = same_a[iu]\n",
    "    B = same_b[iu]\n",
    "    inter = np.sum(A & B)\n",
    "    union = np.sum(A | B)\n",
    "    return np.nan if union == 0 else inter / union\n",
    "\n",
    "def collect_solutions_for_K(clusters_dict, K):\n",
    "    \"\"\"\n",
    "    clusters_dict structure:\n",
    "      {cluster_count: {n_neighbors: {min_samples: labels_ndarray}}}\n",
    "    Returns list of (n_neighbors, min_samples, labels) for the specified K.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    if K not in clusters_dict:\n",
    "        return out\n",
    "    for nn, inner in clusters_dict[K].items():\n",
    "        for ms, labels in inner.items():\n",
    "            out.append(((nn, ms), np.asarray(labels)))\n",
    "    return out\n",
    "\n",
    "def pairwise_metrics_for_K(clusters_dict, K, ignore_noise=True):\n",
    "    sols = collect_solutions_for_K(clusters_dict, K)\n",
    "    records = []\n",
    "    for (key_i, lab_i), (key_j, lab_j) in combinations(sols, 2):\n",
    "        ari = adjusted_rand_score(lab_i, lab_j)\n",
    "        nmi = normalized_mutual_info_score(lab_i, lab_j)\n",
    "        jac = _jaccard_coassignment(lab_i, lab_j, ignore_noise=ignore_noise)\n",
    "        records.append({\n",
    "            \"K\": K,\n",
    "            \"ref\": key_i,\n",
    "            \"cmp\": key_j,\n",
    "            \"ARI\": ari,\n",
    "            \"NMI\": nmi,\n",
    "            \"Jaccard_coassign\": jac\n",
    "        })\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    return df.sort_values([\"ARI\", \"NMI\"], ascending=False, ignore_index=True)\n",
    "\n",
    "def reference_vs_others_for_K(clusters_dict, K, ref_key, ignore_noise=True):\n",
    "    \"\"\"\n",
    "    ref_key = (n_neighbors, min_samples) of your chosen solution.\n",
    "    Returns DataFrame comparing reference to all other solutions at K.\n",
    "    \"\"\"\n",
    "    sols = dict(collect_solutions_for_K(clusters_dict, K))  # {(nn, ms): labels}\n",
    "    if ref_key not in sols:\n",
    "        raise ValueError(f\"Reference key {ref_key} not found for K={K}.\")\n",
    "    lab_ref = sols[ref_key]\n",
    "    rows = []\n",
    "    for key, lab in sols.items():\n",
    "        if key == ref_key:\n",
    "            continue\n",
    "        ari = adjusted_rand_score(lab_ref, lab)\n",
    "        nmi = normalized_mutual_info_score(lab_ref, lab)\n",
    "        jac = _jaccard_coassignment(lab_ref, lab, ignore_noise=ignore_noise)\n",
    "        rows.append({\"K\": K, \"ref\": ref_key, \"cmp\": key, \"ARI\": ari, \"NMI\": nmi, \"Jaccard_coassign\": jac})\n",
    "    return pd.DataFrame(rows).sort_values([\"ARI\", \"NMI\"], ascending=False, ignore_index=True)\n",
    "\n",
    "def summarize_metrics(df):\n",
    "    \"\"\"\n",
    "    Takes either the pairwise or reference-vs-others DataFrame and returns\n",
    "    mean, median, and IQR for each metric.\n",
    "    \"\"\"\n",
    "    def iqr(x):\n",
    "        return np.nanpercentile(x, 75) - np.nanpercentile(x, 25)\n",
    "    summary = {\n",
    "        \"mean_ARI\": df[\"ARI\"].mean(),\n",
    "        \"median_ARI\": df[\"ARI\"].median(),\n",
    "        \"IQR_ARI\": iqr(df[\"ARI\"]),\n",
    "        \"mean_NMI\": df[\"NMI\"].mean(),\n",
    "        \"median_NMI\": df[\"NMI\"].median(),\n",
    "        \"IQR_NMI\": iqr(df[\"NMI\"]),\n",
    "        \"mean_Jaccard\": df[\"Jaccard_coassign\"].mean(),\n",
    "        \"median_Jaccard\": df[\"Jaccard_coassign\"].median(),\n",
    "        \"IQR_Jaccard\": iqr(df[\"Jaccard_coassign\"])\n",
    "    }\n",
    "    return pd.Series(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for K in sorted(clusters_dict.keys()):\n",
    "    for NN in sorted(clusters_dict[K].keys()):\n",
    "        print(f\"K={K}, n_neighbors={NN}, solutions={clusters_dict[K][NN].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 15  # for example\n",
    "# (1) All-pairs stability at fixed K\n",
    "#df_pairs = pairwise_metrics_for_K(clusters_dict, K, ignore_noise=True)\n",
    "#stab_summary = summarize_metrics(df_pairs)\n",
    "\n",
    "# (2) Reference-vs-others at fixed K\n",
    "ref_key = (100, 22)  # (n_neighbors, min_samples) for your chosen solution\n",
    "df_ref = reference_vs_others_for_K(clusters_dict, K, ref_key, ignore_noise=True)\n",
    "ref_summary = summarize_metrics(df_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ref)\n",
    "print(ref_summary)\n",
    "\n",
    "plt.plot(df_ref.index, df_ref[\"ARI\"], marker='o', label='Mean ARI')\n",
    "plt.violinplot(df_ref[\"ARI\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN with UMAP with Projection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "import pickle\n",
    "\n",
    "vectors = fv_reduced[\"l2\"]\n",
    "\n",
    "umap_model = umap.UMAP(\n",
    "n_components=6,\n",
    "n_neighbors=100,\n",
    "min_dist=0,\n",
    "random_state=42,\n",
    "metric=\"cosine\"\n",
    ").fit_transform(vectors)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=100, min_samples=22, metric=\"cosine\", store_centers=\"medoid\").fit(umap_model)\n",
    "hdbscan_model.labels_ = utils.relabel_by_size(hdbscan_model.labels_)\n",
    "\n",
    "path_to_save = base_dir + '/outputs'\n",
    "filename = 'hdbscan_l2norm_90pca_6components_100nn_0dist_cosine_42randseed_100_minclustsize_22minsamples.pkl'\n",
    "with open(os.path.join(path_to_save, filename), 'wb') as file:\n",
    "    pickle.dump(hdbscan_model, file)\n",
    "\n",
    "for i in range(-1, max(hdbscan_model.labels_)+1):\n",
    "    n_points = np.sum(hdbscan_model.labels_ == i)\n",
    "    print(f\"Cluster {i}: {n_points} points\")\n",
    "\n",
    "\n",
    "umap2d = plot_umap(vectors,\n",
    "                    model_dict,\n",
    "                    show_images=False,\n",
    "                    zoom=0.3,\n",
    "                    n_neighbors=100,\n",
    "                    min_dist=0)\n",
    "\n",
    "utils.print_projection(hdbscan_model.labels_, model_dict, umap2d, show_images=False, negative_silhouette=False, clusters_per_marker=None, zoom=0.4)\n",
    "utils.print_projection(hdbscan_model.labels_, model_dict, umap2d, show_images=True, negative_silhouette=False, clusters_per_marker=None, zoom=0.4)\n",
    "\n",
    "#utils.print_clusters(hdbscan_model.labels_, vectors, model_dict, random_sample=200, print_filename=False, silhouette=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap2d = plot_umap(vectors,\n",
    "                    model_dict,\n",
    "                    plot_2d=False,\n",
    "                    show_images=False,\n",
    "                    zoom=0.3,\n",
    "                    n_neighbors=100,\n",
    "                    min_dist=0)\n",
    "\n",
    "print(np.shape(umap2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "import hdbscan\n",
    "\n",
    "vectors = fv_reduced[\"l2\"]\n",
    "\n",
    "umap_model = umap.UMAP(\n",
    "n_components=6,\n",
    "n_neighbors=100,\n",
    "min_dist=0,\n",
    "random_state=42,\n",
    "metric=\"cosine\"\n",
    ").fit_transform(vectors)\n",
    "\n",
    "# umap_embedding is the 2‑D array returned by UMAP\n",
    "umap_model = np.asarray(umap_model, dtype=np.float64)\n",
    "\n",
    "\"\"\"# scikit‑learn version: add 1 to min_cluster_size and min_samples\n",
    "sk_model = sklearn.cluster.HDBSCAN(min_cluster_size=100, min_samples=22,\n",
    "                                   metric=\"cosine\").fit(umap_model)\"\"\"\n",
    "\n",
    "hdbscan_model= hdbscan.HDBSCAN(min_cluster_size=99,\n",
    "                    min_samples=21,\n",
    "                    metric=\"cosine\",\n",
    "                    algorithm=\"generic\").fit(umap_model)\n",
    "\n",
    "for i in range(-1, max(hdbscan_model.labels_)+1):\n",
    "    n_points = np.sum(hdbscan_model.labels_ == i)\n",
    "    print(f\"Cluster {i}: {n_points} points\")\n",
    "\n",
    "\"\"\"for i in range(-1, max(sk_model.labels_)+1):\n",
    "    n_points = np.sum(sk_model.labels_ == i)\n",
    "    print(f\"Cluster {i}: {n_points} points\")\n",
    "    \"\"\"\n",
    "print(validity_index(umap_model, hdbscan_model.labels_, metric=\"cosine\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import glasbey\n",
    "hdbscan_model.condensed_tree_.plot(leaf_separation=1,\n",
    "select_clusters=True,\n",
    "label_clusters=True,\n",
    "log_size=True,\n",
    "max_rectangles_per_icicle=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdbscan_model.cluster_persistence_)\n",
    "print(hdbscan_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.base import clone\n",
    "from sklearn.cluster import KMeans\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.offsetbox import (AnnotationBbox, DrawingArea, OffsetImage,\n",
    "                                  TextArea)\n",
    "from matplotlib.textpath import TextPath\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# Image Directories\n",
    "image_dir = 'Images/DM_Images_sorted_cropped_448'\n",
    "\n",
    "path_to_save = base_dir + '/outputs'\n",
    "\n",
    "# HDBSCAN model path\n",
    "hdbscan_path = 'hdbscan_l2norm_90pca_6components_100nn_0dist_cosine_42randseed_100_minclustsize_22minsamples.pkl'\n",
    "\n",
    "with open(os.path.join(path_to_save, hdbscan_path), 'rb') as file:\n",
    "    hdbscan_model = pickle.load(file)\n",
    "\n",
    "hdbscan_model.labels_ = utils.relabel_by_size(hdbscan_model.labels_)\n",
    "\n",
    "# UMAP embeddings paths\n",
    "umap_model_path = 'umap_embeddings_6d_100nn.pkl'\n",
    "umap_2d_path = 'umap_embeddings_2d_100nn.pkl'\n",
    "umap_2d_150_path = 'umap_embeddings_2d_150nn.pkl'\n",
    "\n",
    "with open(os.path.join(path_to_save, umap_model_path), \"rb\") as file:\n",
    "    umap_model = pkl.load(file)\n",
    "\n",
    "with open(os.path.join(path_to_save, umap_2d_path), \"rb\") as file:\n",
    "    umap_2d = pkl.load(file)\n",
    "\n",
    "with open(os.path.join(path_to_save, umap_2d_150_path), \"rb\") as file:\n",
    "    umap_2d_150 = pkl.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cluster Drawings in Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'clusters'\n",
    "os.makedirs(dir, exist_ok=False)\n",
    "\n",
    "model_dict['filenames']\n",
    "image_dir = model_dict['image_dir']\n",
    "\n",
    "# Group filenames and distances by cluster\n",
    "clustered_filenames = {}\n",
    "\n",
    "# Group filenames by cluster\n",
    "for filename, label in zip(filenames, hdbscan_model.labels_):\n",
    "    if label not in clustered_filenames:\n",
    "        clustered_filenames[label] = []\n",
    "    clustered_filenames[label].append(filename)\n",
    "\n",
    "# Sort the dictionaries by the length of the lists in their keys\n",
    "sorted_clusters = sorted(clustered_filenames.items(), key=lambda item: len(item[1]))\n",
    "\n",
    "# Group filenames by cluster\n",
    "for cluster, images in clustered_filenames.items():\n",
    "\n",
    "    cluster_dir = f'{dir}/cluster_{cluster}'\n",
    "\n",
    "    os.makedirs(cluster_dir, exist_ok=False)\n",
    "\n",
    "    for filename in images:\n",
    "        image = cv2.imread(os.path.join(image_dir, filename))\n",
    "        cv2.imwrite(f'{cluster_dir}/{filename}', image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Print the size of each cluster in the current k-means clustering result\n",
    "\n",
    "unique_labels, counts = np.unique(hdbscan_model.labels_, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Cluster {label}: {count} points\")\n",
    "\n",
    "utils.print_clusters(hdbscan_model.labels_, vectors, model_dict, random_sample=100, print_filename=False, silhouette=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.gridspec as gridspec\n",
    "from collections import defaultdict\n",
    "\n",
    "# Labels\n",
    "labels_path = config[\"project_paths\"]['labels'] # path to the labels file\n",
    "labels_df = pd.read_excel(labels_path, sheet_name='Sheet1')\n",
    "\n",
    "labels_df['cluster'] = pd.to_numeric(labels_df['cluster'], errors='coerce').round().astype('Int64')\n",
    "labels_df = labels_df.dropna(subset=['cluster']).copy()\n",
    "labels_df['cluster'] = labels_df['cluster'].astype(int)\n",
    "labels_df['category'] = labels_df['category'].astype(str).str.strip().str.lower()\n",
    "\n",
    "n_clusters = len(set(hdbscan_model.labels_))\n",
    "\n",
    "label_order = ['kluverian', 'non-kluverian', 'non-geometric', 'miscellaneous', \"noise\"]\n",
    "\n",
    "for label in label_order:\n",
    "    print(label)\n",
    "    clusters_for_label = labels_df[labels_df['category'] == label]['cluster'].tolist()\n",
    "    num_images = labels_df[labels_df['category'] == label]['quantity'].sum()\n",
    "    print(f'# Clusters = {len(clusters_for_label)}, {len(clusters_for_label)/n_clusters*100:.2f}% of total clusters')\n",
    "    print(f'# Images = , {num_images}, {num_images/len(filenames)*100:.2f}% of total images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import website\n",
    "import json as std_json\n",
    "\n",
    "\n",
    "data = website.build_embeddings_scattergl_demo(\n",
    "        umap=umap_2d_150,\n",
    "        labels=hdbscan_model.labels_,\n",
    "        labels_df=labels_df\n",
    "        )\n",
    "\n",
    "with open(\"dm_data.json\", \"w\") as file:\n",
    "    string_data = std_json.dumps(data)\n",
    "    \n",
    "    file.write(string_data)\n",
    "print(\"Data saved to data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP DataMapPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamapplot\n",
    "import requests\n",
    "import io\n",
    "\n",
    "hdbscan_description_labels = []\n",
    "\n",
    "hdbscan_model.labels_ = utils.relabel_by_size(hdbscan_model.labels_)\n",
    "\n",
    "for label in hdbscan_model.labels_:\n",
    "    if label == -1:\n",
    "        hdbscan_description_labels.append('Unlabelled')\n",
    "    else:\n",
    "        hdbscan_description_labels.append(label)\n",
    "    \"\"\"else:\n",
    "        description = labels_df.loc[labels_df['cluster'] == label, 'category']\n",
    "        hdbscan_description_labels.append(str(description.iloc[0]))\"\"\"\n",
    "\n",
    "print(len(hdbscan_description_labels))\n",
    "\n",
    "print(umap_model)\n",
    "\n",
    "datamapplot.create_plot(umap_model,\n",
    "                        hdbscan_description_labels,\n",
    "                        color_label_text=False,\n",
    "                        label_over_points=True,\n",
    "                        label_font_size=24,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_projection(hdbscan_model.labels_,\n",
    "                       model_dict,\n",
    "                       umap2d,\n",
    "                       show_images=True,\n",
    "                       with_filenames=True,\n",
    "                       zoom=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.pprint_projection(hdbscan_model.labels_,\n",
    "                       model_dict,\n",
    "                       umap_2d_150,\n",
    "                       image_labels=\"selected\",\n",
    "                       cluster_descriptions=labels_df,\n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       clusters_per_marker=None,\n",
    "                       zoom=1.6,\n",
    "                       #probability_desaturate=False,\n",
    "                       #hdbscan_model=None\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"123,456382349/3333\"\n",
    "bad = {' ': 0, ',': 1, '/': 1}  # value = chars to INCLUDE when breaking at that char\n",
    "width = 10\n",
    "\n",
    "def wrap_prefer_breaks(s: str, bad_map={' ': 0, ',': 1, '/': 1}, max_width=10) -> str:\n",
    "    lines = []\n",
    "    line_start = 0\n",
    "    last_break = None       # index of last seen break char\n",
    "    last_break_keep = 0     # how many chars to include from the break\n",
    "\n",
    "    for i, ch in enumerate(s):\n",
    "        # track preferred breakpoints\n",
    "        if ch in bad_map:\n",
    "            last_break = i\n",
    "            last_break_keep = bad_map[ch]\n",
    "\n",
    "        # if we've exceeded width, break\n",
    "        if i - line_start + 1 > max_width:\n",
    "            if last_break is not None and last_break >= line_start:\n",
    "                cut = last_break + last_break_keep\n",
    "                lines.append(s[line_start:cut])\n",
    "                line_start = last_break + 1\n",
    "                last_break = None\n",
    "                last_break_keep = 0\n",
    "            else:\n",
    "                # no good breakpoint in window: hard break before current char\n",
    "                lines.append(s[line_start:i])\n",
    "                line_start = i\n",
    "\n",
    "    # add the remainder\n",
    "    if line_start < len(s):\n",
    "        lines.append(s[line_start:])\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "description_new = wrap_prefer_breaks(description, bad, width)\n",
    "print(description_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glasbey\n",
    "\n",
    "glasbey.create_palette(palette_size=15, chroma_bounds=(60, 100), lightness_bounds=(30, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Order and Figure Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_order = ['kluverian', 'non-kluverian', 'non-geometric', 'miscellaneous', \"noise\"]\n",
    "\n",
    "background_color = {'kluverian': \"#caeafb\",\n",
    "                    'non-kluverian': \"#dad6fe\",\n",
    "                    'non-geometric': \"#e4fdce\",\n",
    "                    'miscellaneous': \"#fcdbcc\",\n",
    "                    'noise': \"#dfdede\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_figs(num_columns=6,\n",
    "                    num_clust_columns=1,\n",
    "                    image_selection=\"medoid\",\n",
    "                    seed=42,\n",
    "                    include_noise=True):\n",
    "    \"\"\"\n",
    "    image_selection:\n",
    "      - \"medoid\"/\"mediod\": nearest to cluster medoid (first image is medoid)\n",
    "      - \"random\": random order, reproducible with `seed`\n",
    "    Excludes cluster -1.\n",
    "    \"\"\"\n",
    "    # RNG: re-init every call => identical output for same seed\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # grid sanity\n",
    "    assert isinstance(num_columns, int) and num_columns >= 2\n",
    "    assert isinstance(num_clust_columns, int) and num_clust_columns >= 1\n",
    "\n",
    "    # labels & filenames\n",
    "    clusters_ = hdbscan_model.labels_.astype(int)\n",
    "    filenames = np.asarray(model_dict['filenames'])\n",
    "    image_dir = model_dict['image_dir']\n",
    "\n",
    "    # feature space for medoid computation\n",
    "    try:\n",
    "        X = umap_model  # embedding array from your Cell 2\n",
    "    except NameError:\n",
    "        X = vectors\n",
    "    if X.shape[0] != len(filenames):\n",
    "        X = vectors  # fallback if mismatch\n",
    "\n",
    "    # cluster -> indices (exclude -1)\n",
    "    cluster_indices = defaultdict(list)\n",
    "    for i, lbl in enumerate(clusters_):\n",
    "        if include_noise and lbl == -1:\n",
    "            cluster_indices[int(lbl)].append(i)\n",
    "        elif lbl != -1:\n",
    "            cluster_indices[int(lbl)].append(i)\n",
    "\n",
    "    # build per-cluster ordered filename lists\n",
    "    clustered_filenames = {}\n",
    "\n",
    "    if image_selection == \"medoid\":\n",
    "        for lbl in sorted(cluster_indices.keys()):   # sorted => stable\n",
    "            idxs = cluster_indices[lbl]\n",
    "            Xi = X[idxs]\n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "            # Euclidean medoid\n",
    "            diff = Xi[:, None, :] - Xi[None, :, :]\n",
    "            D = np.linalg.norm(diff, axis=2)\n",
    "            medoid_local = np.argmin(D.sum(axis=1))\n",
    "            d_to_medoid = D[:, medoid_local]\n",
    "            order = np.argsort(d_to_medoid)          # medoid first\n",
    "            clustered_filenames[lbl] = [filenames[idxs[k]] for k in order]\n",
    "        note_text = \"closest to\\nmedoid\"\n",
    "    elif image_selection == \"random\":\n",
    "        for lbl in sorted(cluster_indices.keys()):   # sorted => stable across calls\n",
    "            idxs = cluster_indices[lbl]\n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "            order = rng.permutation(len(idxs))       # uses our local, re-seeded RNG\n",
    "            clustered_filenames[lbl] = [filenames[idxs[k]] for k in order]\n",
    "        note_text = \"\"\n",
    "    else:\n",
    "        raise ValueError(\"image_selection must be 'medoid'/'mediod' or 'random'\")\n",
    "\n",
    "    # plot: one figure per label (labels_df already sorted by size)\n",
    "    for category in label_order:\n",
    "        target_clusters = labels_df.loc[labels_df['category'] == category, 'cluster'].astype(int).tolist()\n",
    "        target_clusters = [c for c in target_clusters if c != -1 or include_noise]\n",
    "\n",
    "        num_rows = len(target_clusters)\n",
    "        if num_rows == 0:\n",
    "            continue\n",
    "\n",
    "        fig = plt.figure(figsize=(num_clust_columns * num_columns, num_rows),\n",
    "                         facecolor=background_color.get(category, \"white\"))\n",
    "        outer_grid = gridspec.GridSpec(num_rows, num_clust_columns, wspace=0.02, hspace=0.1)\n",
    "        clusters_plotted = 0\n",
    "\n",
    "        for cluster_id in target_clusters:\n",
    "            images = clustered_filenames.get(cluster_id, [])\n",
    "            if not images:\n",
    "                continue\n",
    "\n",
    "            col = (clusters_plotted // num_rows)\n",
    "            row = (clusters_plotted % num_rows)\n",
    "            inner_grid = gridspec.GridSpecFromSubplotSpec(\n",
    "                1, num_columns, subplot_spec=outer_grid[row, col], wspace=0.1, hspace=0\n",
    "            )\n",
    "            clusters_plotted += 1\n",
    "\n",
    "            for plot_idx in range(num_columns):\n",
    "                ax = fig.add_subplot(inner_grid[0, plot_idx])\n",
    "\n",
    "                if clusters_plotted == 1 and plot_idx == 0:\n",
    "                    if cluster_id != -1:\n",
    "                        ax.text(0.5, 1.13, \"Cluster\", fontsize=10, ha='center', va='bottom', fontstyle='italic')\n",
    "\n",
    "                    ax.text(1.6, 1.1, note_text, fontsize=10, ha='center', va='bottom', fontstyle='italic')\n",
    "\n",
    "                if plot_idx == 0:\n",
    "                    dser = labels_df.loc[labels_df['cluster'] == cluster_id, 'description']\n",
    "                    desc = dser.iloc[0] if not dser.empty else None\n",
    "                    include_desc = (isinstance(desc, str) and desc.strip() != \"\") and not pd.isna(desc)\n",
    "\n",
    "                    text = f\"{cluster_id}\\nN={len(images)}\" #if not include_desc else f\"{cluster_id}\\n{desc.strip()}\\nN={len(images)}\"\n",
    "                    ax.text(0.98, 0.5, text, ha='right', va='center',\n",
    "                            fontsize=10, transform=ax.transAxes, wrap=True)\n",
    "                    ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "                    ax.margins(0); ax.axis('off')\n",
    "                else:\n",
    "                    img_idx = plot_idx - 1\n",
    "                    if img_idx < len(images):\n",
    "                        fname = images[img_idx]\n",
    "                        img = cv2.imread(os.path.join(image_dir, str(fname)))\n",
    "                        if img is not None:\n",
    "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                            ax.imshow(img)\n",
    "                    ax.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "plot_label_figs(num_columns=6,\n",
    "                num_clust_columns=1,\n",
    "                image_selection=\"random\",\n",
    "                seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location and Attendance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Image Directories\n",
    "image_dir = 'Images/DM_Images_sorted_cropped_448'\n",
    "\n",
    "model_name = 'dino'\n",
    "kmeans = {}\n",
    "\n",
    "vectors = fv_reduced[\"l2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Location Frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata_path = config[\"project_paths\"]['metadata']\n",
    "metadata_df = pd.read_excel(metadata_path, sheet_name='Sheet1')\n",
    "\n",
    "metadata_labeled_path = config[\"project_paths\"]['metadata_labeled']\n",
    "metadata_labeled_df = pd.read_excel(metadata_labeled_path, sheet_name='Sheet1')\n",
    "\n",
    "metadata_labeled_df['is_inlier'] = metadata_labeled_df['filename'].isin(model_dict[\"filenames\"])\n",
    "metadata_df['is_inlier'] = metadata_df['filename'].isin(model_dict[\"filenames\"])\n",
    "\n",
    "print(\"NO LOCATION DATA: \", metadata_labeled_df['is_inlier'].value_counts())\n",
    "print(\"METADATA: \", metadata_df['is_inlier'].value_counts())\n",
    "\n",
    "# We are missing metadata for 67 images\n",
    "\n",
    "location_frequencies = metadata_df[metadata_df['is_inlier'] == True]['location'].value_counts()\n",
    "location_frequencies_percentage = location_frequencies / location_frequencies.sum() * 100  # Convert to percentage\n",
    "print(\"Location Frequencies:\\n\", location_frequencies)\n",
    "print(\"Location Frequencies (Percentage):\\n\", location_frequencies_percentage)\n",
    "\n",
    "metadata_df.fillna({\"location\": 'Unknown'}, inplace=True)\n",
    "print(metadata_df['location'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterimining Location for Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and adding clusters to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metadata_path = config[\"project_paths\"]['metadata']\n",
    "metadata_df = pd.read_excel(metadata_path, sheet_name='Sheet1')\n",
    "\n",
    "not_in_clusterd_filenames = []\n",
    "\n",
    "new_labels = utils.relabel_by_size(hdbscan_model.labels_)\n",
    "\n",
    "for filename, label in zip(filenames, new_labels):\n",
    "\n",
    "    label = int(label)\n",
    "\n",
    "    metadata_df.loc[metadata_df['filename'] == filename, 'cluster'] = label # add cluster data if filename exists in metadata_df\n",
    "\n",
    "    # If filename not in metadata_df, add new row with filename and cluster data\n",
    "    if filename not in metadata_df['filename'].values:\n",
    "        metadata_df = pd.concat([metadata_df, pd.DataFrame({'filename': [filename], 'cluster': [label]})], ignore_index=True)\n",
    "\n",
    "metadata_df['cluster'] = metadata_df['cluster'].astype('Int64')\n",
    "\n",
    "display(metadata_df)\n",
    "\n",
    "print(metadata_df['cluster'].value_counts(sort=False))\n",
    "print(metadata_df['cluster'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Squared Test\n",
    "\n",
    "The chi-squared p-value will be technically valid only if you treat clusters as if they were fixed categories (like “treatment A vs treatment B”), not if you treat clustering as part of the randomness.\n",
    "In other words: if your scientific question is “given the clusters we defined, is location independent of cluster?”, then is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create a contingency table of cluster vs location\n",
    "filtered_df = metadata_df[metadata_df['cluster'].notna()]\n",
    "contingency = pd.crosstab(filtered_df['cluster'], filtered_df['location'], dropna=False)\n",
    "\n",
    "# Perform Chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "print(f\"Chi-squared: {chi2}, p-value: {p}, Degrees of freedom: {dof}\")\n",
    "\n",
    "\n",
    "# Create a contingency with totals\n",
    "contingency_totals = pd.crosstab(filtered_df['cluster'], filtered_df['location'], dropna=False)\n",
    "contingency_totals['Sum'] = contingency.sum(axis=1)\n",
    "contingency_totals.loc['Sum'] = contingency.sum(axis=0)\n",
    "print(\"\\nContingency Table with Totals:\")\n",
    "display(contingency_totals)\n",
    "\n",
    "# Create a contingency table normalized by rows (percentage values)\n",
    "percentage_contingency = pd.crosstab(filtered_df['cluster'], filtered_df['location'], dropna=False, normalize='index') * 100\n",
    "percentage_contingency = percentage_contingency.round(2)\n",
    "\n",
    "overall_total = metadata_df['cluster'].count()\n",
    "row_pct = ((contingency.sum(axis=1) / overall_total) * 100).round(2)\n",
    "col_pct = ((contingency.sum(axis=0) / overall_total) * 100).round(2)\n",
    "percentage_contingency['RowTotal'] = row_pct\n",
    "col_pct['Sum'] = 100.0\n",
    "percentage_contingency.loc['Sum'] = col_pct\n",
    "\n",
    "print(\"\\nPercentage Contingency Table with Totals:\")\n",
    "display(percentage_contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Squared Without No location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create a contingency table of cluster vs location\n",
    "contingency = pd.crosstab(metadata_df['cluster'], metadata_df['location'])\n",
    "\n",
    "# Perform Chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "print(f\"Chi-squared: {chi2}, p-value: {p}, Degrees of freedom: {dof}\")\n",
    "\n",
    "\n",
    "# Create a contingency with totals\n",
    "contingency_totals = pd.crosstab(metadata_df['cluster'], metadata_df['location'])\n",
    "contingency_totals['Sum'] = contingency.sum(axis=1)\n",
    "contingency_totals.loc['Sum'] = contingency.sum(axis=0)\n",
    "print(\"\\nContingency Table with Totals:\")\n",
    "display(contingency_totals)\n",
    "\n",
    "# Create a contingency table normalized by rows (percentage values)\n",
    "percentage_contingency = pd.crosstab(metadata_df['cluster'], metadata_df['location'], normalize='index') * 100\n",
    "percentage_contingency = percentage_contingency.round(2)\n",
    "\n",
    "overall_total = metadata_df['cluster'].count()\n",
    "row_pct = ((contingency.sum(axis=1) / overall_total) * 100).round(2)\n",
    "col_pct = ((contingency.sum(axis=0) / overall_total) * 100).round(2)\n",
    "percentage_contingency['RowTotal'] = row_pct\n",
    "col_pct['Sum'] = 100.0\n",
    "percentage_contingency.loc['Sum'] = col_pct\n",
    "\n",
    "print(\"\\nPercentage Contingency Table with Totals:\")\n",
    "display(percentage_contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted Pearson (Standardized) Residuals\n",
    "\n",
    "Resource: https://cscu.cornell.edu/wp-content/uploads/conttableresid.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "# standardized residuals = (observed - expected) / sqrt(expected)\n",
    "residuals = (contingency - expected) / np.sqrt(expected*(1 - contingency.sum(axis=1).values[:, None]/contingency.values.sum())*(1 - contingency.sum(axis=0).values[None, :]/contingency.values.sum()))\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "sns.heatmap(residuals.round(2), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Standardized Residuals Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "bonferrioni_threshold = 0.05 / (contingency.shape[0] * contingency.shape[1])\n",
    "critical_value = norm.ppf(1 - bonferrioni_threshold/2)\n",
    "print(f\"Bonferroni-corrected significance threshold: {bonferrioni_threshold}\")\n",
    "print(f\"Critical Chi-squared value for Bonferroni correction: {critical_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing all combinations of cluster and location, it was seen that images in cluster 1 from London are obseved signficantly more than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect Size - Cramer's V\n",
    "\n",
    "If V is, say, <0.1, that’s a very weak association — i.e. statistically significant but practically negligible.\n",
    "\n",
    "“Although the chi² test indicates a significant difference from the global distribution (p=0.006), the effect size is negligible (Cramér’s V=0.046), suggesting the location distribution is approximately stable across clusters.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = contingency.values.sum()\n",
    "cramer_v = np.sqrt(chi2 / (n * (min(contingency.shape)-1)))\n",
    "print(\"Cramer's V:\", cramer_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Location Attendance and Session Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HS & DL Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "session_attendance_path = config[\"project_paths\"]['session_attendance_data']\n",
    "session_attendance_df = pd.read_excel(session_attendance_path, sheet_name='Sheet1')\n",
    "\n",
    "# Session counts and percentages\n",
    "session_counts = session_attendance_df['session'].value_counts()\n",
    "print(\"Session Counts:\", session_counts)\n",
    "print(\"Total:\", session_attendance_df['session'].count())\n",
    "session_percentage = session_counts / session_counts.sum() * 100\n",
    "print(\"Session Percentage:\")\n",
    "print(session_percentage)\n",
    "\n",
    "# Attendance counts by session type\n",
    "session_type_counts = session_attendance_df.groupby('session')['scanned'].sum()\n",
    "print(\"\\nAttendance per Session Counts:\", session_type_counts)\n",
    "print(\"Total:\", session_attendance_df['scanned'].count())\n",
    "print(\"Attendance per Session Percentage:\")\n",
    "print(session_type_counts / session_attendance_df['scanned'].sum() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session counts and percentages\n",
    "location_counts = session_attendance_df['location'].value_counts()\n",
    "print(\"Location Counts:\", location_counts)\n",
    "print(\"Total:\", session_attendance_df['location'].count())\n",
    "location_percentage = location_counts / location_counts.sum() * 100\n",
    "print(\"Location Percentage:\")\n",
    "print(location_percentage)\n",
    "\n",
    "\n",
    "# Attendance counts by session type\n",
    "session_type_counts = session_attendance_df.groupby('location')['scanned'].sum()\n",
    "print(\"\\nAttendance per Session Counts:\", session_type_counts)\n",
    "print(\"Total:\", session_attendance_df['scanned'].count())\n",
    "print(\"Attendance per Session Percentage:\")\n",
    "print(session_type_counts / session_attendance_df['scanned'].sum() * 100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOpXIqel0/Udyy3R5hyYQb/",
   "collapsed_sections": [
    "ZBKJoqKWeSef"
   ],
   "machine_shape": "hm",
   "mount_file_id": "1JFH7BmvGhRv5HSva_FRyktovMPc82inK",
   "provenance": [
    {
     "file_id": "1JFH7BmvGhRv5HSva_FRyktovMPc82inK",
     "timestamp": 1713362514493
    },
    {
     "file_id": "1OJoMHdWX5Pic54E36q_VBMusVE_74BC2",
     "timestamp": 1713278963502
    },
    {
     "file_id": "15abn9VjpZBid4zZpjCWzJml2y1c1j04e",
     "timestamp": 1712846752724
    },
    {
     "file_id": "1ep8E2gtkWd2kdMZh9vkQqYU0iLsC6c2M",
     "timestamp": 1697909616183
    },
    {
     "file_id": "19Q8KjTbwuKkY6M9B17UKCS12UttnB7XN",
     "timestamp": 1692812186304
    },
    {
     "file_id": "1DjgUERAuRpfGhqVW0o3i6oHB7PknuWyC",
     "timestamp": 1690639241935
    },
    {
     "file_id": "15JKoqCCy3lW_dAagO0HtIT-5wMK1dH01",
     "timestamp": 1689537341085
    },
    {
     "file_id": "1c9nB1IMRoi_YhF5ac8LqdIuOjlTQ7RSk",
     "timestamp": 1688908057222
    },
    {
     "file_id": "1ct9qVjyUTEOXL9gFjSxjuERFFK-t5F4b",
     "timestamp": 1688481381883
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "flicker_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05f6a3fd356948a4b90de817eee4cc21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cd3830992704f8694ea467c3e9c1e0c",
      "placeholder": "​",
      "style": "IPY_MODEL_8c2c6403a2db48b98e3dfe4c95ecaf11",
      "value": "model.safetensors: 100%"
     }
    },
    "14d18137f8444a0082fef3951ae0d870": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cd3830992704f8694ea467c3e9c1e0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "252a0e03f5354a13932d4ad8988e947f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_375265f50de341f8aec7d5c74758bf44",
      "max": 4519,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d15179b254354a7ca45b7f59ffc7a435",
      "value": 4519
     }
    },
    "2859e68afd4d4640aac1c719ac9ed88d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "375265f50de341f8aec7d5c74758bf44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56225b7f4e384b3288df92d28702d5a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6928e1c0972144bc9755b3eb21793241": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91dea5305a574a4183e0a809b0100aa0",
      "placeholder": "​",
      "style": "IPY_MODEL_741162d62d1447389b0cf11ec7f34b29",
      "value": " 1.71G/1.71G [00:05&lt;00:00, 345MB/s]"
     }
    },
    "6a6874b916bf4b009c1ea3a88806e09e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14d18137f8444a0082fef3951ae0d870",
      "placeholder": "​",
      "style": "IPY_MODEL_bed37d66f49d452f8e49c1600e3177bc",
      "value": "config.json: 100%"
     }
    },
    "741162d62d1447389b0cf11ec7f34b29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7da701efc5444c0097dd093f04495138": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56225b7f4e384b3288df92d28702d5a1",
      "placeholder": "​",
      "style": "IPY_MODEL_2859e68afd4d4640aac1c719ac9ed88d",
      "value": " 4.52k/4.52k [00:00&lt;00:00, 366kB/s]"
     }
    },
    "8c2c6403a2db48b98e3dfe4c95ecaf11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91dea5305a574a4183e0a809b0100aa0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9474008a4d36437ca215460ca8f74e32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "990c5c04750945f382232c05b8bc81ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_05f6a3fd356948a4b90de817eee4cc21",
       "IPY_MODEL_de08b87aa44144c097ff8978dcf31807",
       "IPY_MODEL_6928e1c0972144bc9755b3eb21793241"
      ],
      "layout": "IPY_MODEL_d9f20fd02d81447cb6b7cc1c32d69c3c"
     }
    },
    "b8e5123528ac4b1f8577e283a224a019": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bed37d66f49d452f8e49c1600e3177bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf4117ee01cd4a4e9b17334d56025b89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a6874b916bf4b009c1ea3a88806e09e",
       "IPY_MODEL_252a0e03f5354a13932d4ad8988e947f",
       "IPY_MODEL_7da701efc5444c0097dd093f04495138"
      ],
      "layout": "IPY_MODEL_9474008a4d36437ca215460ca8f74e32"
     }
    },
    "d15179b254354a7ca45b7f59ffc7a435": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d2d9cbfaf9704cc0a87e2effcca12298": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9f20fd02d81447cb6b7cc1c32d69c3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de08b87aa44144c097ff8978dcf31807": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2d9cbfaf9704cc0a87e2effcca12298",
      "max": 1710540580,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b8e5123528ac4b1f8577e283a224a019",
      "value": 1710540580
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
