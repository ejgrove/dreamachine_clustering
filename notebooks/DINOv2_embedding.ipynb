{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiZSydwzPm5v"
      },
      "source": [
        "This script was run in google colab on an A100 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oevKkbUDfYT7",
        "outputId": "1fea813b-689d-46e2-a749-c255769fe76a"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xEIDFoQVUiX",
        "outputId": "6e41df48-f252-4121-8e00-581313b438f7"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install xformers\n",
        "!pip install triton\n",
        "\n",
        "!pip freeze > requirements.txt\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtutrLxcQIJP"
      },
      "source": [
        "# Importing Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuKqbqOHQIJT"
      },
      "source": [
        "### DINOv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OIrOArOQIJT",
        "outputId": "0248ace1-811c-4d94-f65f-737f1973795e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# import dinov2_vitl14_reg model\n",
        "dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')\n",
        "\n",
        "# have model go into evaluation mode\n",
        "dino_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJXpm0cTQIJT"
      },
      "source": [
        "# Importing Libraries and Setup\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBgonQA6QIJT",
        "outputId": "2aa100d1-efbf-462a-c35b-9977dbc0a159"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from numpy import random as rand\n",
        "from pathlib import Path\n",
        "\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import cv2\n",
        "import skimage\n",
        "from skimage import exposure\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.amp.autocast_mode import autocast\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision.io import read_image\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# Get current date\n",
        "date = datetime.now().strftime('%d%m%Y')\n",
        "\n",
        "### EDIT PATHS ###\n",
        "PROJECT_DIR = Path(\"/content/drive/MyDrive/YOUR_PROJECT_FOLDER\")\n",
        "DATASET_PATH = PROJECT_DIR / \"preprocessed_images\" # Set dreamachine drawing path\n",
        "\n",
        "# Create folder for saving feature vectors\n",
        "OUTPUT_DIR = PROJECT_DIR / \"embeddings\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set number of workers for dataloader\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Set device to GPU if available (We used A100 GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def randseed(seed):\n",
        "    ''' Set random seed for reproducibility '''\n",
        "\n",
        "    # Set random seed for Python\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set random seed for PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    # Set random seed for cuDNN\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "randseed(SEED)\n",
        "\n",
        "# Set pytorch to be deterministic/reproducible\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Print device and number of workers\n",
        "print(\"Number of workers:\", NUM_WORKERS)\n",
        "print('Images in dataset: ', len(os.listdir(DATASET_PATH)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IuoUElOS9aG"
      },
      "source": [
        "Move the dreamachine images to temporary memory for faster upload to the GPU. (Runtime ~10 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juziSXVX04lc",
        "outputId": "76164bee-efc4-4180-c5da-2a4a4fc62782"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copytree(DATASET_PATH, Path(\"/dev/shm/dataset\"))\n",
        "DATASET_PATH = Path(\"/dev/shm/dataset\")\n",
        "\n",
        "print('Images in dataset: ', len(os.listdir(DATASET_PATH)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt3TVEDwAY8k"
      },
      "source": [
        "# Preprocessing & Augmentation Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UisxXwZQIJU"
      },
      "outputs": [],
      "source": [
        "# Set Parameters\n",
        "dino_params = {'norm_mean': (0.485, 0.456, 0.406), # Standard ImageNet Params (https://github.com/facebookresearch/dinov2/issues/181)\n",
        "              'norm_std_dev': (0.229, 0.224, 0.225),\n",
        "              'image_size': 448,\n",
        "              'feature_length':1024}\n",
        "\n",
        "def equalize(tensor):\n",
        "    ''' Equalize the histogram of an image to make exposure uniform'''\n",
        "\n",
        "    # Get dimensions of PyTorch tensor in the format C x H x W\n",
        "    height, width = tensor.shape[1], tensor.shape[2]\n",
        "\n",
        "    np_image = tensor.permute(1, 2, 0).numpy() # Convert tensor to numpy array\n",
        "    np_image = exposure.equalize_adapthist(np_image, clip_limit=0.03) # Clip limit determined via testing...\n",
        "    np_image = torch.tensor(np_image).permute(2, 0, 1) # Convert back to tensor\n",
        "    return np_image.float()  # Ensure tensor is float for subsequent processing\n",
        "\n",
        "def preprocess_transforms(image, model):\n",
        "    ''' Preprocess image for DINOv2 model '''\n",
        "    transformation = v2.Compose([v2.ToImage(), # Convert to PIL Image\n",
        "                                 v2.ToDtype(torch.uint8, scale=True), # Convert to uint8\n",
        "                                 v2.Lambda(lambda x: equalize(x)), # Histogram Equalization\n",
        "                                 v2.GaussianBlur(kernel_size=(7, 7), sigma=(1)), # Gaussian Blur\n",
        "                                 v2.ToDtype(torch.float32, scale=True) # Convert to float32\n",
        "    ])\n",
        "    return transformation(image)\n",
        "\n",
        "class TestTimeAugmentations:\n",
        "    ''' test time augmentations '''\n",
        "\n",
        "    def __init__(self, num_transforms=5, model=None, norm=True):\n",
        "        ''' Initialize test time augmentations\n",
        "\n",
        "            num_transforms: number of augmentations to apply\n",
        "            model: model parameters\n",
        "            norm:normalize image\n",
        "        '''\n",
        "\n",
        "        self.num_transforms = num_transforms\n",
        "        self.model = model\n",
        "        self.norm = norm\n",
        "\n",
        "    def transform(self, image):\n",
        "        ''' Apply set of transforms to image '''\n",
        "\n",
        "        # Start with a list containing just the original image (1 img)\n",
        "        images = [image]\n",
        "\n",
        "        # Horizontally flip the image and add to list (2 imgs)\n",
        "        images.extend([v2.functional.hflip(img) for img in images])\n",
        "\n",
        "        # Apply inversion to images in list (4 imgs)\n",
        "        images.extend([v2.functional.invert(img) for img in images])\n",
        "\n",
        "        # Rotate at all 90 degree positions to images in list (16 imgs)\n",
        "        images = [v2.functional.rotate(img, angle) for img in images for angle in [0, 90, 180, 270]]\n",
        "\n",
        "        # Set seed for color jitter such that all images get same same color jitter\n",
        "        torch.manual_seed(SEED)\n",
        "\n",
        "        # Define random color jitter augmentation\n",
        "        jitter = v2.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n",
        "\n",
        "        # Apply jitter to images in list (16 * num_transforms imgs)\n",
        "        images = [jitter(img) for i in range(self.num_transforms) for img in images]\n",
        "\n",
        "        # Normalize images\n",
        "        if self.norm:\n",
        "            normalize = v2.Normalize(mean=self.model['norm_mean'], std=self.model['norm_std_dev'])\n",
        "            images = [normalize(img) for img in images]\n",
        "\n",
        "        return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZl-mRIoQIJU"
      },
      "source": [
        "### Visualization Of Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "68Yc2aE0QIJV",
        "outputId": "1fe4c5aa-2814-4578-c79a-e9bba71a8e8c"
      },
      "outputs": [],
      "source": [
        "def show(image):\n",
        "    ''' Show image '''\n",
        "\n",
        "    # Create a ToPILImage transform\n",
        "    pil_image = v2.ToPILImage()(image)\n",
        "\n",
        "    plt.imshow(pil_image)\n",
        "    plt.show\n",
        "\n",
        "# directory\n",
        "dir = os.listdir(DATASET_PATH)\n",
        "\n",
        "# set random seed\n",
        "randseed(SEED)\n",
        "\n",
        "# pick random image from directory and show\n",
        "image = read_image(os.path.join(DATASET_PATH, random.choice(dir)))\n",
        "show(image)\n",
        "\n",
        "# set parameters\n",
        "params = dino_params\n",
        "\n",
        "# set number of transforms and augmentations\n",
        "num_transforms = 1\n",
        "num_augmenations = 16*num_transforms\n",
        "\n",
        "# preprocess and show\n",
        "preprocess = preprocess_transforms(image, params)\n",
        "show(preprocess)\n",
        "\n",
        "# apply test time augmentations\n",
        "augmentation = TestTimeAugmentations(num_transforms=num_transforms, model=params, norm=False)\n",
        "\n",
        "images = augmentation.transform(preprocess)\n",
        "\n",
        "# show images\n",
        "for j in range(int(len(images)/16)):\n",
        "\n",
        "    images_idx = images[j*16:(j+1)*16]\n",
        "\n",
        "    plt.figure(figsize=(12,12))\n",
        "\n",
        "    for idx, image in enumerate(images_idx):\n",
        "\n",
        "        plt.subplot(4,4,idx+1)\n",
        "\n",
        "        #Create a ToPILImage transform\n",
        "        pil_image = v2.ToPILImage()(image)\n",
        "\n",
        "        plt.axis('off')\n",
        "        plt.imshow(pil_image)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grIh540-QIJV"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDsc8WuAQIJV"
      },
      "source": [
        "### Define Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB4eGz6KQIJV"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "class MyImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, model, preprocess_transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.preprocess_transform = preprocess_transform\n",
        "        self.img_files = os.listdir(root_dir)\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.root_dir, self.img_files[idx])\n",
        "        image = read_image(image_path)\n",
        "\n",
        "        if self.preprocess_transform:\n",
        "            image = self.preprocess_transform(image, model=self.model)\n",
        "\n",
        "        return image, self.img_files[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czYmP7KKQIJW"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "\n",
        "def ExtractFeatureVectors(model,\n",
        "                          dataloader,\n",
        "                          model_params,\n",
        "                          model_name,\n",
        "                          num_augmentations=1,\n",
        "                          feature_vector_name=None):\n",
        "        \"Run dataset through model and save feature vectors to disk\"\n",
        "\n",
        "        labels = []\n",
        "\n",
        "        randseed(SEED)\n",
        "\n",
        "        # Empty Tensor for feature vectors\n",
        "        feature_vectors = torch.empty((0, model_params['feature_length']), device=device)\n",
        "\n",
        "        # Define test time augmentations\n",
        "        augmentation_transforms = TestTimeAugmentations(num_transforms=num_augmentations,\n",
        "                                                        model=model_params)\n",
        "\n",
        "        # Process the images in batches\n",
        "        for idx, (inputs, names) in enumerate(tqdm(dataloader)):\n",
        "\n",
        "                augmented_images = []\n",
        "\n",
        "                # For image in each batch\n",
        "                for i in range(inputs.shape[0]):\n",
        "\n",
        "                    # Apply test time augmentations\n",
        "                    augmented_images.extend(augmentation_transforms.transform(inputs[i]))\n",
        "\n",
        "                    # Add label for each image\n",
        "                    labels.extend([names[i]] * num_augmentations * 16)\n",
        "\n",
        "                # Stack all augmented_images\n",
        "                batch = torch.stack(augmented_images)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    with autocast(\"cuda\"):\n",
        "\n",
        "                        # Inference\n",
        "                        output = model(batch.to(device=\"cuda\"))\n",
        "\n",
        "                        # Store the embeddings in the feature_vectors tensor\n",
        "                        feature_vectors = torch.cat((feature_vectors, output), dim=0)\n",
        "\n",
        "        # Move feature_vectors tensor to cpu\n",
        "        feature_vectors = feature_vectors.cpu()\n",
        "\n",
        "        # Save all feature vectors\n",
        "        torch.save(\n",
        "            feature_vectors,\n",
        "            str(OUTPUT_DIR / f\"feature_vectors_all_augments_{feature_vector_name}_{model_name}_{num_augmentations*16}_augmentations_{date}.pt\"),\n",
        "        )\n",
        "\n",
        "        # Save all feature vectors dictionary\n",
        "        feature_vectors_dict = defaultdict(list)\n",
        "\n",
        "        for idx, i in enumerate(labels):\n",
        "            feature_vectors_dict[i].append(feature_vectors[idx])\n",
        "\n",
        "        torch.save(\n",
        "            feature_vectors_dict,\n",
        "            str(OUTPUT_DIR / f\"feature_vectors_dict_all_augments_{feature_vector_name}_{model_name}_{num_augmentations*16}_augmentations_{date}.pt\"),\n",
        "        )\n",
        "\n",
        "        # Save average feature vectors dictionary\n",
        "        feature_vector_dict_mean = {}\n",
        "        for key, value in feature_vectors_dict.items():\n",
        "\n",
        "            fv = torch.stack(value)\n",
        "\n",
        "            feature_vector_dict_mean[key] = torch.mean(fv, dim=0)\n",
        "\n",
        "        torch.save(\n",
        "            feature_vector_dict_mean,\n",
        "            str(OUTPUT_DIR / f\"feature_vectors_dict_{feature_vector_name}_{model_name}_{num_augmentations*16}_augmentations_{date}.pt\"),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i34extoWwKt4"
      },
      "source": [
        "runtime (58:20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WSQ6NppfMiXF",
        "outputId": "311d48bf-82b7-465d-fe53-12e901895d70"
      },
      "outputs": [],
      "source": [
        "# Define GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Empty GPU Cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Set model and parameters\n",
        "model = dino_model\n",
        "model_params = dino_params\n",
        "model_name =  'dino'\n",
        "\n",
        "# Set feature vector filename\n",
        "feature_vector_name = 'DM_full'\n",
        "\n",
        "# Set model float type\n",
        "model.to(device=device, dtype=torch.float16)\n",
        "\n",
        "# Number of augmentations (*16) per image and batch size (*num_augmentations*16 images total)\n",
        "num_augmentations = 3\n",
        "batch_size = 3\n",
        "\n",
        "# Define dataset\n",
        "dataset = MyImageFolder(root_dir=DATASET_PATH,\n",
        "                        model = model_params,\n",
        "                        preprocess_transform=preprocess_transforms)\n",
        "\n",
        "# Define dataloader\n",
        "dataloader = DataLoader(dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=NUM_WORKERS,\n",
        "                        pin_memory=True)\n",
        "\n",
        "# Run Inference on Images\n",
        "ExtractFeatureVectors(model,\n",
        "                      dataloader,\n",
        "                      model_params,\n",
        "                      model_name,\n",
        "                      num_augmentations=num_augmentations,\n",
        "                      feature_vector_name=feature_vector_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
