{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736fdefa",
   "metadata": {},
   "source": [
    "# Location and Attendance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500046d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path.cwd().parent\n",
    "sys.path.insert(0, str(base_dir))\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "### EDIT PATHS ###\n",
    "base_dir = Path.cwd().parent\n",
    "image_dir = \"/path/to/preprocessed_images\"\n",
    "attendance_path = \"/path/to/attendance.xlsx\"  #(not public)\n",
    "\n",
    "# Repo-relative paths (do not change)\n",
    "cache_dir = base_dir / \"outputs\" / \"cache\"\n",
    "labels_path = base_dir / \"data\" / \"cluster_labels.xlsx\"\n",
    "metadata_path = base_dir / \"data\" / \"image_preprocessing_labels.xlsx\"\n",
    "hdbscan_path = \"hdbscan_l2norm_90pca_6components_100nn_0dist_cosine_42randseed_100_minclustsize_22minsamples.pkl\"\n",
    "\n",
    "filenames = os.listdir(image_dir)\n",
    "\n",
    "with open(cache_dir / hdbscan_path, \"rb\") as file:\n",
    "    hdbscan_model = pkl.load(file)\n",
    "\n",
    "hdbscan_model.labels_ = utils.relabel_by_size(hdbscan_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35979ec3",
   "metadata": {},
   "source": [
    "## Location Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6bc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_labeled_df = pd.read_excel(metadata_path, sheet_name=\"Sheet1\")\n",
    "metadata_labeled_df[\"is_inlier\"] = metadata_labeled_df[\"filename\"].isin(filenames)\n",
    "\n",
    "print(\"METADATA: \", metadata_labeled_df[\"is_inlier\"].value_counts())\n",
    "\n",
    "# NO METADATA FOR 67 IMAGES\n",
    "\n",
    "location_frequencies = metadata_labeled_df[metadata_labeled_df[\"is_inlier\"] == True][\"location\"].value_counts()\n",
    "location_frequencies_percentage = location_frequencies / location_frequencies.sum() * 100\n",
    "print(\"Location Frequencies:\\n\", location_frequencies)\n",
    "print(\"Location Frequencies (Percentage):\\n\", location_frequencies_percentage)\n",
    "\n",
    "metadata_labeled_df.fillna({\"location\": \"Unknown\"}, inplace=True)\n",
    "print(metadata_labeled_df[\"location\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806dd9c5",
   "metadata": {},
   "source": [
    "## Assign Clusters to Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d680421",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = utils.relabel_by_size(hdbscan_model.labels_)\n",
    "\n",
    "for filename, label in zip(filenames, new_labels):\n",
    "    label = int(label)\n",
    "    metadata_labeled_df.loc[metadata_labeled_df[\"filename\"] == filename, \"cluster\"] = label\n",
    "    if filename not in metadata_labeled_df[\"filename\"].values:\n",
    "        metadata_labeled_df = pd.concat(\n",
    "            [metadata_labeled_df, pd.DataFrame({\"filename\": [filename], \"cluster\": [label]})],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "metadata_labeled_df[\"cluster\"] = metadata_labeled_df[\"cluster\"].astype(\"Int64\")\n",
    "\n",
    "display(metadata_labeled_df)\n",
    "print(metadata_labeled_df[\"cluster\"].value_counts(sort=False))\n",
    "print(metadata_labeled_df[\"cluster\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d15ea",
   "metadata": {},
   "source": [
    "## Chi-Squared Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef609c",
   "metadata": {},
   "source": [
    "\n",
    "### Not including unknown location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create a contingency table of cluster vs location\n",
    "filtered_df = metadata_labeled_df[metadata_labeled_df[\"cluster\"].notna()]\n",
    "contingency = pd.crosstab(filtered_df[\"cluster\"], filtered_df[\"location\"], dropna=False)\n",
    "\n",
    "# Perform Chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "print(f\"Chi-squared: {chi2}, p-value: {p}, Degrees of freedom: {dof}\")\n",
    "\n",
    "# Create a contingency with totals\n",
    "contingency_totals = pd.crosstab(filtered_df[\"cluster\"], filtered_df[\"location\"], dropna=False)\n",
    "contingency_totals[\"Sum\"] = contingency.sum(axis=1)\n",
    "contingency_totals.loc[\"Sum\"] = contingency.sum(axis=0)\n",
    "print(\"\\nContingency Table with Totals:\")\n",
    "display(contingency_totals)\n",
    "\n",
    "# Create a contingency table normalized by rows (percentage values)\n",
    "percentage_contingency = pd.crosstab(filtered_df[\"cluster\"], filtered_df[\"location\"], dropna=False, normalize=\"index\") * 100\n",
    "percentage_contingency = percentage_contingency.round(2)\n",
    "\n",
    "overall_total = metadata_labeled_df[\"cluster\"].count()\n",
    "row_pct = ((contingency.sum(axis=1) / overall_total) * 100).round(2)\n",
    "col_pct = ((contingency.sum(axis=0) / overall_total) * 100).round(2)\n",
    "percentage_contingency[\"RowTotal\"] = row_pct\n",
    "col_pct[\"Sum\"] = 100.0\n",
    "percentage_contingency.loc[\"Sum\"] = col_pct\n",
    "\n",
    "print(\"\\nPercentage Contingency Table with Totals:\")\n",
    "display(percentage_contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc9636",
   "metadata": {},
   "source": [
    "### Including Uknown Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create a contingency table of cluster vs location\n",
    "contingency = pd.crosstab(metadata_labeled_df['cluster'], metadata_labeled_df['location'])\n",
    "\n",
    "# Perform Chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "print(f\"Chi-squared: {chi2}, p-value: {p}, Degrees of freedom: {dof}\")\n",
    "\n",
    "\n",
    "# Create a contingency with totals\n",
    "contingency_totals = pd.crosstab(metadata_labeled_df['cluster'], metadata_labeled_df['location'])\n",
    "contingency_totals['Sum'] = contingency.sum(axis=1)\n",
    "contingency_totals.loc['Sum'] = contingency.sum(axis=0)\n",
    "print(\"\\nContingency Table with Totals:\")\n",
    "display(contingency_totals)\n",
    "\n",
    "# Create a contingency table normalized by rows (percentage values)\n",
    "percentage_contingency = pd.crosstab(metadata_labeled_df['cluster'], metadata_labeled_df['location'], normalize='index') * 100\n",
    "percentage_contingency = percentage_contingency.round(2)\n",
    "\n",
    "overall_total = metadata_labeled_df['cluster'].count()\n",
    "row_pct = ((contingency.sum(axis=1) / overall_total) * 100).round(2)\n",
    "col_pct = ((contingency.sum(axis=0) / overall_total) * 100).round(2)\n",
    "percentage_contingency['RowTotal'] = row_pct\n",
    "col_pct['Sum'] = 100.0\n",
    "percentage_contingency.loc['Sum'] = col_pct\n",
    "\n",
    "print(\"\\nPercentage Contingency Table with Totals:\")\n",
    "display(percentage_contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d7cc0",
   "metadata": {},
   "source": [
    "## Adjusted Pearson (Standardized) Residuals\n",
    "\n",
    "Resource: https://cscu.cornell.edu/wp-content/uploads/conttableresid.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "residuals = (contingency - expected) / np.sqrt(\n",
    "    expected *\n",
    "    (1 - contingency.sum(axis=1).values[:, None] / contingency.values.sum()) *\n",
    "    (1 - contingency.sum(axis=0).values[None, :] / contingency.values.sum())\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "sns.heatmap(residuals.round(2), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Standardized Residuals Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "bonferrioni_threshold = 0.05 / (contingency.shape[0] * contingency.shape[1])\n",
    "critical_value = norm.ppf(1 - bonferrioni_threshold/2)\n",
    "print(f\"Bonferroni-corrected significance threshold: {bonferrioni_threshold}\")\n",
    "print(f\"Critical Chi-squared value for Bonferroni correction: {critical_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844228e",
   "metadata": {},
   "source": [
    "## Effect Size (Cramer's V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = contingency.values.sum()\n",
    "cramer_v = np.sqrt(chi2 / (n * (min(contingency.shape)-1)))\n",
    "print(\"Cramer's V:\", cramer_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b232ae",
   "metadata": {},
   "source": [
    "## Attendance Analysis (Not Public)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f830940",
   "metadata": {},
   "source": [
    "### HS & DL Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901030f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session Attendance Analysis\n",
    "if attendance_path is None:\n",
    "    raise FileNotFoundError(\"Set attendance_path to run this section.\")\n",
    "\n",
    "session_attendance_df = pd.read_excel(attendance_path, sheet_name='Sheet1')\n",
    "\n",
    "# Session counts and percentages\n",
    "session_counts = session_attendance_df['session'].value_counts()\n",
    "print(\"Session Counts:\", session_counts)\n",
    "print(\"Total:\", session_attendance_df['session'].count())\n",
    "session_percentage = session_counts / session_counts.sum() * 100\n",
    "print(\"Session Percentage:\")\n",
    "print(session_percentage)\n",
    "\n",
    "# Attendance counts by session type\n",
    "session_type_counts = session_attendance_df.groupby('session')['scanned'].sum()\n",
    "print(\"\\nAttendance per Session Counts:\", session_type_counts)\n",
    "print(\"Total:\", session_attendance_df['scanned'].count())\n",
    "print(\"Attendance per Session Percentage:\")\n",
    "print(session_type_counts / session_attendance_df['scanned'].sum() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497e3ce",
   "metadata": {},
   "source": [
    "### By Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ad583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session counts and percentages\n",
    "location_counts = session_attendance_df['location'].value_counts()\n",
    "print(\"Session Counts:\", location_counts)\n",
    "print(\"Total:\", session_attendance_df['location'].count())\n",
    "location_percentage = location_counts / location_counts.sum() * 100\n",
    "print(\"Session Percentage:\")\n",
    "print(location_percentage)\n",
    "\n",
    "# Attendance counts by session type\n",
    "session_type_counts = session_attendance_df.groupby('location')['scanned'].sum()\n",
    "print(\"\\nAttendance per Session Counts:\", session_type_counts)\n",
    "print(\"Total:\", session_attendance_df['scanned'].count())\n",
    "print(\"Attendance per Session Percentage:\")\n",
    "print(session_type_counts / session_attendance_df['scanned'].sum() * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flicker_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
